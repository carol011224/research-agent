import streamlit as st
import os
import json
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.agents import create_agent
from langchain_core.messages import SystemMessage, HumanMessage
import requests
import feedparser

# æ˜¯å¦ä½¿ç”¨ Agentï¼ˆLLM è‡ªä¸»é¸æ“‡å·¥å…·ï¼‰ã€‚é è¨­ Falseï¼Œä¿æŒå›ºå®šé †åºã€‚
USE_AGENT = False

# ----------------------
# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
# ----------------------
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    st.error("âŒ ç„¡æ³•è®€å– OPENAI_API_KEY")
    st.stop()

# ----------------------
# Streamlit è¨­å®š
# ----------------------
st.set_page_config(page_title="LangChain Research Agent", layout="wide")
st.title("ğŸ¤– LangChain Research Agent")
st.write("AI Agent ç ”ç©¶ç³»çµ±ï¼šTopic Refiner â†’ Researcher â†’ Summarizer")

with st.sidebar:
    topic = st.text_input("Research topic", value="æœ€æ–° RAG çš„æ–¹æ³•")
    st.markdown("---")

# ----------------------
# LangChain LLM åˆå§‹åŒ–
# ----------------------
llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.1,
    streaming=False,
    api_key=api_key
)

# ----------------------
# arXiv æœå°‹å‡½æ•¸
# ----------------------
def search_arxiv(query, max_results=3, start_offset=0):
    try:
        clean_query = query.replace('?', '').replace('!', '').replace('ã€‚', '').replace('ï¼Ÿ', '').strip()
        query_words = clean_query.split()
        if len(query_words) > 5:
            clean_query = ' '.join(query_words[:5])
        url = "https://export.arxiv.org/api/query"
        params = {
            'search_query': f'ti:{clean_query} OR abs:{clean_query}',
            'start': start_offset,
            'max_results': max_results,
            'sortBy': 'submittedDate',
            'sortOrder': 'descending'
        }
        headers = {'User-Agent': 'Mozilla/5.0', 'Accept': 'application/atom+xml'}
        response = requests.get(url, params=params, headers=headers, timeout=15)
        response.raise_for_status()
        feed = feedparser.parse(response.content)
        papers = []
        for entry in feed.entries:
            paper_id = entry.id.split('/')[-1]
            summary = entry.summary.replace('\n', ' ').strip()
            if len(summary) > 500:
                summary = summary[:500] + "..."
            papers.append({
                'title': entry.title,
                'authors': [author.name for author in entry.authors],
                'summary': summary,
                'published': entry.published,
                'arxiv_id': paper_id,
                'url': entry.id,
                'categories': [tag.term for tag in entry.tags]
            })
        return papers
    except Exception as e:
        print(f"arXiv æœå°‹å¤±æ•—: {e}")
        return []

# ----------------------
# å°è£æˆ LangChain Tools
# ----------------------
def topic_refiner_tool(input_topic: str) -> str:
    """å°‡ç ”ç©¶ä¸»é¡Œç´°åŒ–ç‚º 3-5 å€‹å…·é«”å­å•é¡Œä¸¦é™„ clarifierï¼Œè¼¸å‡º JSON å­—ä¸²ã€‚"""
    prompt = (
        f"ä½ æ˜¯ä¸€åè³‡æ·±ç ”ç©¶åŠ©ç†ã€‚é‡å°ä¸»é¡Œã€Œ{input_topic}ã€ï¼Œç”Ÿæˆ 3~5 å€‹å…·é«”ã€å¯ç ”ç©¶çš„å­å•é¡Œï¼Œ"
        "æ¯å€‹å­å•é¡Œéœ€åŒ…å« question èˆ‡ clarifierã€‚åªè¼¸å‡º JSONï¼Œä¸è¦åŠ å¤šé¤˜æ–‡å­—ã€‚\n"
        "JSON ç¯„ä¾‹ï¼š\n"
        "[\n  {\n    \"question\": \"{å­å•é¡Œä¸€}\",\n    \"clarifier\": \"{è£œå……èªªæ˜}\"\n  },\n  {\n    \"question\": \"{å­å•é¡ŒäºŒ}\",\n    \"clarifier\": \"{è£œå……èªªæ˜}\"\n  }\n]"
    )
    resp = llm.invoke([
        SystemMessage(content="You are a helpful research assistant."),
        HumanMessage(content=prompt)
    ])
    return getattr(resp, "content", str(resp))

def researcher_tool(questions_json: str) -> str:
    """æ ¹æ“šå­å•é¡Œ JSONï¼ŒæŸ¥è©¢çœŸå¯¦ arXiv è«–æ–‡ä¸¦ç”¢å‡ºåˆ†æçµæœï¼Œè¼¸å‡º JSON å­—ä¸²ã€‚"""
    questions = json.loads(questions_json)
    results = []
    seen_ids = set()
    for i, q in enumerate(questions):
        question = q.get("question", "")
        # å°é½Š app_langchainï¼šæ¯é¡Œä½¿ç”¨ä¸åŒ offset æé«˜å»é‡æ•ˆæœ
        start_offset = i * 5
        papers = search_arxiv(question, max_results=5, start_offset=start_offset)
        # éæ¿¾é‡è¤‡
        unique_papers = [p for p in papers if p['arxiv_id'] not in seen_ids]
        for p in unique_papers:
            seen_ids.add(p['arxiv_id'])
        # åªå–å‰ä¸‰ç¯‡
        unique_papers = unique_papers[:3]
        # ä½¿ç”¨ LLM åˆ†æè«–æ–‡
        if unique_papers:
            prompt = (
                "ä½ æ˜¯ä¸€åç ”ç©¶å“¡ã€‚é‡å°å­å•é¡Œï¼š" + question + "\n"
                "ä»¥ä¸‹ç‚ºçœŸå¯¦çš„ arXiv è«–æ–‡è³‡æ–™ï¼ˆtitle/authors/summary/published/url/categoriesï¼‰ï¼š\n" +
                json.dumps(unique_papers, ensure_ascii=False, indent=2) + "\n\n"
                "è«‹åªè¼¸å‡º JSONï¼ˆä¸è¦å¤šé¤˜æ–‡å­—ï¼‰ï¼Œçµæ§‹å¦‚ä¸‹ï¼š\n"
                "{\n  \"findings\": [\"åŸºæ–¼è«–æ–‡çš„ç™¼ç¾1\", \"åŸºæ–¼è«–æ–‡çš„ç™¼ç¾2\"],\n"
                "  \"sources\": [\"arXiv è«–æ–‡æ¨™é¡Œæˆ–é€£çµ\"],\n"
                "  \"confidence\": \"low|medium|high\",\n"
                "  \"data_quality\": \"good|limited\",\n"
                "  \"notes\": \"è£œå……èªªæ˜ï¼ˆå¦‚é™åˆ¶/ä¸è¶³ï¼‰\"\n}"
            )
        else:
            prompt = (
                "ä½ æ˜¯ä¸€åç ”ç©¶å“¡ã€‚é‡å°å­å•é¡Œï¼š" + question + " æœªæ‰¾åˆ°ç›¸é—œ arXiv è«–æ–‡ã€‚\n"
                "è«‹åªè¼¸å‡º JSONï¼ˆä¸è¦å¤šé¤˜æ–‡å­—ï¼‰ï¼Œçµæ§‹å¦‚ä¸‹ï¼š\n"
                "{\n  \"findings\": [\"ä¸€èˆ¬çŸ¥è­˜çš„ç™¼ç¾1\", \"ä¸€èˆ¬çŸ¥è­˜çš„ç™¼ç¾2\"],\n"
                "  \"sources\": [\"ä¸€èˆ¬çŸ¥è­˜\"],\n"
                "  \"confidence\": \"low\",\n"
                "  \"data_quality\": \"limited\",\n"
                "  \"notes\": \"æœªæ‰¾åˆ°å­¸è¡“è«–æ–‡ï¼Œåƒ…æä¾›ä¸€èˆ¬çŸ¥è­˜\"\n}"
            )
        resp = llm.invoke([
            SystemMessage(content="You are a helpful research assistant."),
            HumanMessage(content=prompt)
        ])
        raw = getattr(resp, "content", str(resp))
        results.append({"question": question, "analysis": raw, "arxiv_papers": unique_papers})
    return json.dumps(results, ensure_ascii=False)

def summarizer_tool(research_results_json: str) -> str:
    """æ ¹æ“šç ”ç©¶çµæœ JSON ç”Ÿæˆå®Œæ•´ç ”ç©¶å ±å‘Šï¼ˆâ‰¤600å­—ï¼‰ï¼Œè¿”å›ç´”æ–‡å­—ã€‚"""
    prompt = (
        "ä½ æ˜¯ä¸€åç§‘å­¸ä½œå®¶ã€‚è«‹æ ¹æ“šä»¥ä¸‹ç ”ç©¶çµæœï¼ˆå«æ¯å€‹å­å•é¡Œçš„ findings/sources/confidence/data_quality/notesï¼‰æ’°å¯«æœ€çµ‚ç ”ç©¶å ±å‘Šã€‚\n"
        + research_results_json + "\n\n"
        "è«‹ä»¥ Markdown è¼¸å‡ºï¼Œä¸¦ä½¿ç”¨ä¸­æ–‡å°ç¯€æ¨™é¡Œï¼ˆä½œç‚ºå‰¯æ¨™é¡Œï¼‰å¦‚ä¸‹ï¼š\n"
        "### æ‘˜è¦\n(ç´„ 3 è¡Œ)\n\n"
        "### å¼•è¨€\n(1 æ®µ)\n\n"
        "### ä¸»è¦ç™¼ç¾\n(æ•´åˆæ¯å€‹å­å•é¡Œçš„é‡é»æ¢åˆ—)\n\n"
        "### é™åˆ¶\n(1 æ®µ)\n\n"
        "### å»ºè­°çš„ä¸‹ä¸€æ­¥\n(3 æ¢æ¢åˆ—)\n\n"
        "å­—æ•¸ â‰¤ 600ï¼›åªè¼¸å‡ºä¸Šè¿° Markdown å…§å®¹ï¼Œä¸è¦å¤šé¤˜èªªæ˜æˆ– JSONã€‚"
    )
    resp = llm.invoke([
        SystemMessage(content="You are a helpful research assistant."),
        HumanMessage(content=prompt)
    ])
    return getattr(resp, "content", str(resp))

# ----------------------
# å®šç¾© Tools
# ----------------------
tools = [
    topic_refiner_tool,
    researcher_tool,
    summarizer_tool
]

# ----------------------
# åˆå§‹åŒ– session stateï¼ˆæ¯”ç…§ app_langchainï¼‰
# ----------------------
if 'research_completed' not in st.session_state:
    st.session_state.research_completed = False
if 'research_data' not in st.session_state:
    st.session_state.research_data = {}
if 'current_topic' not in st.session_state:
    st.session_state.current_topic = ""

# ----------------------
# Main UIï¼ˆæ¯”ç…§ app_langchainï¼‰
# ----------------------
if st.button("Start Research ğŸš€"):
    # æ¸…ç©ºä¹‹å‰çš„çµæœ
    st.session_state.research_completed = False
    st.session_state.research_data = {}
    st.session_state.current_topic = topic

    st.subheader("ğŸ¤– LangChain Agent åŸ·è¡Œ")

    # é€²åº¦é¡¯ç¤º
    progress_bar = st.progress(0)
    status_text = st.empty()

    try:
        if USE_AGENT:
            # ä½¿ç”¨ LangChain Agent è®“ LLM è‡ªä¸»æŒ‘é¸å·¥å…·
            status_text.text("ğŸ¤– æ­£åœ¨ä»¥ Agent æ¨¡å¼åŸ·è¡Œ...")
            progress_bar.progress(30)
            agent = create_agent(model=llm, tools=tools, system_prompt="You are a helpful research assistant.")
            result = agent.invoke({"messages": [{"role": "user", "content": topic}]})
            # ç°¡å–®æŠ½å–å›è¦†æ–‡å­—
            result_report = ""
            if isinstance(result, dict):
                msgs = result.get("messages")
                if isinstance(msgs, list):
                    for m in reversed(msgs):
                        if isinstance(m, dict) and m.get("role") == "assistant" and isinstance(m.get("content"), str):
                            result_report = m["content"]
                            break
            if not result_report:
                result_report = str(result)

            # Agent æ¨¡å¼ä¸‹ï¼Œç¶­æŒ UIï¼šåƒ…å±•ç¤ºæœ€çµ‚å ±å‘Š
            questions = []
            research_results = []
            raw_build = ""
            final_report = result_report
            status_text.text("âœ… Agent åŸ·è¡Œå®Œæˆï¼")
            progress_bar.progress(100)
        else:
            # å›ºå®šé †åºï¼šTopic â†’ Researcher â†’ Summarizer
            status_text.text("ğŸ—ï¸ æ­£åœ¨ç´°åŒ–ä¸»é¡Œ (Topic Refiner)...")
            progress_bar.progress(20)

            topic_json = topic_refiner_tool(topic)
            # è§£æå•é¡Œåˆ—è¡¨ï¼Œåƒè€ƒ app_langchain çš„é‚è¼¯
            questions: list = []
            raw_build = topic_json
            try:
                parsed = json.loads(topic_json.strip())
                if isinstance(parsed, dict) and 'sub_questions' in parsed:
                    questions_list = parsed['sub_questions']
                elif isinstance(parsed, list):
                    questions_list = parsed
                else:
                    questions_list = []
                for i, q in enumerate(questions_list, 1):
                    if isinstance(q, dict):
                        questions.append({
                            "id": f"Q{i}",
                            "question": q.get("question", ""),
                            "clarifier": q.get("clarifier", "")
                        })
            except Exception:
                questions = []

            if not questions:
                # å¾Œå‚™ï¼šç°¡å–®ç”Ÿæˆä¸‰å€‹æ¨£æ¿å•é¡Œ
                questions = [
                    {"id": "Q1", "question": f"{topic}çš„åŸºæœ¬åŸç†æ˜¯ä»€éº¼ï¼Ÿ", "clarifier": f"é—œæ–¼ {topic} çš„åŸºæœ¬æ¦‚å¿µ"},
                    {"id": "Q2", "question": f"{topic}çš„æœ€æ–°ç™¼å±•è¶¨å‹¢å¦‚ä½•ï¼Ÿ", "clarifier": f"é—œæ–¼ {topic} çš„ç™¼å±•ç¾æ³"},
                    {"id": "Q3", "question": f"{topic}çš„æ‡‰ç”¨é ˜åŸŸæœ‰å“ªäº›ï¼Ÿ", "clarifier": f"é—œæ–¼ {topic} çš„å¯¦éš›æ‡‰ç”¨"},
                ]

            status_text.text("ğŸ“ æ­£åœ¨æœå°‹ä¸¦åˆ†æè«–æ–‡ (Researcher)...")
            progress_bar.progress(55)

            researcher_json = researcher_tool(json.dumps(questions, ensure_ascii=False))
            research_results = []
            try:
                tmp_list = json.loads(researcher_json)
                for item in tmp_list:
                    analysis_raw = item.get("analysis", "")
                    try:
                        analysis_obj = json.loads(analysis_raw)
                    except Exception:
                        analysis_obj = {
                            "findings": [],
                            "sources": [],
                            "confidence": "unknown",
                            "data_quality": "unknown",
                            "notes": "raw text",
                            "__raw": analysis_raw
                        }
                    analysis_obj['arxiv_papers'] = item.get('arxiv_papers', [])
                    research_results.append({
                        "id": None,
                        "question": item.get("question", ""),
                        "analysis": analysis_obj
                    })
            except Exception:
                research_results = []

            status_text.text("âœï¸ æ­£åœ¨æ•´åˆå ±å‘Š (Summarizer)...")
            progress_bar.progress(80)

            final_report = summarizer_tool(json.dumps(research_results, ensure_ascii=False))

            status_text.text("âœ… Agent åŸ·è¡Œå®Œæˆï¼")
            progress_bar.progress(100)

    except Exception as e:
        status_text.text(f"âŒ Agent åŸ·è¡Œå¤±æ•—: {str(e)}")
        progress_bar.progress(0)
        st.error(f"Agent åŸ·è¡ŒéŒ¯èª¤: {str(e)}")
        st.stop()

    # å±•ç¤ºçµæœï¼ˆæ¯”ç…§ app_langchainï¼‰
    st.subheader("ğŸ—ï¸ Topic Refiner")
    st.markdown("### ğŸ“‹ ç´°åŒ–çš„ç ”ç©¶å•é¡Œ")
    if questions:
        for i, q in enumerate(questions, 1):
            qid = q.get('id', f'Q{i}')
            qtext = q.get('question', '')
            clar = q.get('clarifier', '')
            st.write(f"**ğŸ“Œ {qid}:** {qtext}")
            if clar:
                st.write(f"ğŸ’¡ {clar}")
            st.write("")

    with st.expander("ğŸ” æŸ¥çœ‹åŸå§‹è¼¸å‡º (Raw Output)", expanded=False):
        st.code(raw_build[:3000], language="json")

    st.subheader("ğŸ“ Researcher")
    st.write("**Research results:**")
    for i, result in enumerate(research_results):
        with st.expander(f"ç ”ç©¶å•é¡Œ {i+1}: {result['question']}"):
            analysis = result['analysis']
            st.write("**ç ”ç©¶ç™¼ç¾:**")
            for finding in analysis.get('findings', []):
                st.write(f"â€¢ {finding}")
            st.write("**è³‡æ–™ä¾†æº:**")
            for source in analysis.get('sources', []):
                st.write(f"â€¢ {source}")
            st.write(f"**å¯ä¿¡åº¦:** {analysis.get('confidence', 'unknown')}")
            st.write(f"**è³‡æ–™å“è³ª:** {analysis.get('data_quality', 'unknown')}")
            arxiv_papers = analysis.get('arxiv_papers', [])
            if arxiv_papers:
                st.write("**ğŸ“š arXiv è«–æ–‡:**")
                for paper in arxiv_papers:
                    st.write(f"ğŸ“„ **[{paper['title']}]({paper['url']})**")
                    st.write(f"   ğŸ‘¥ ä½œè€…: {', '.join(paper['authors'][:3])}{'...' if len(paper['authors']) > 3 else ''}")
                    st.write(f"   ğŸ“… ç™¼å¸ƒ: {paper['published'][:10]}")
                    st.write(f"   ğŸ“ æ‘˜è¦: {paper['summary'][:500]}...")
                    st.write(f"   ğŸ·ï¸ åˆ†é¡: {', '.join(paper['categories'][:3])}")
                    st.write("---")
            else:
                st.write("**âš ï¸ æœªæ‰¾åˆ°ç›¸é—œ arXiv è«–æ–‡**")

    with st.expander("ğŸ“Š å®Œæ•´ç ”ç©¶çµæœ (JSON)", expanded=False):
        st.write(research_results)

    st.subheader("âœï¸ Summarizer")
    st.markdown(final_report)

    st.session_state.research_data = {
        'questions': questions,
        'raw_build': raw_build,
        'research_results': research_results,
        'final_report': final_report,
        'topic': topic,
        'agent_output': {
            'questions': questions,
            'research_results': research_results,
            'final_report': final_report
        }
    }

    st.download_button("Download report (.txt)", data=final_report, file_name="research_report.txt")
    st.download_button("Download JSON export", data=json.dumps(st.session_state.research_data, ensure_ascii=False, indent=2), file_name="research_export.json")
    st.session_state.research_completed = True

elif st.session_state.research_completed and st.session_state.research_data:
    data = st.session_state.research_data

    st.subheader("ğŸ—ï¸ Topic Refiner")
    if data['questions']:
        for i, q in enumerate(data['questions'], 1):
            qid = q.get('id', f'Q{i}')
            qtext = q.get('question', '')
            clar = q.get('clarifier', '')
            st.write(f"**ğŸ“Œ {qid}:** {qtext}")
            if clar:
                st.write(f"ğŸ’¡ {clar}")
            st.write("")

    with st.expander("ğŸ” æŸ¥çœ‹åŸå§‹è¼¸å‡º (Raw Output)", expanded=False):
        st.code(data['raw_build'][:3000], language="json")

    st.subheader("ğŸ“ Researcher")
    with st.expander("ğŸ“Š å®Œæ•´ç ”ç©¶çµæœ (JSON)", expanded=False):
        st.write(data['research_results'])

    st.subheader("âœï¸ Summarizer")
    st.markdown(data['final_report'])

    st.download_button("Download report (.txt)", data=data['final_report'], file_name="research_report.txt")
    st.download_button("Download JSON export", data=json.dumps(data, ensure_ascii=False, indent=2), file_name="research_export.json")
