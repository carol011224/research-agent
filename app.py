import streamlit as st
from openai import OpenAI
import json
import os
import requests
import feedparser
import random
from datetime import datetime, timedelta
from time import sleep
from dotenv import load_dotenv

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

st.set_page_config(page_title="A Research Agent Demo", layout="wide")
st.title("A Research Agent Demo")
st.write("Three sub-agents: Topic Refiner â†’ Researcher â†’ Summarizer")

# ----------------------
# æª¢æŸ¥ API Key
# ----------------------
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    st.error("âŒ ç„¡æ³•è®€å– .env æª”æ¡ˆæˆ–æ‰¾ä¸åˆ° OPENAI_API_KEY")
    st.stop()

# ----------------------
# Sidebar: ä¸»é¡Œè¨­å®š
# ----------------------
with st.sidebar:
    topic = st.text_input("Research topic", value="æœ€æ–°å¤šèªè¨€ LLM fine-tuning æ–¹æ³•")
    # temp = st.slider("Temperature", 0.0, 1.0, 0.2) # å¯è®“ä½¿ç”¨è€…é¸æ“‡ temperature åƒæ•¸
    st.markdown("---")

# ----------------------
# LLM wrapper
# ----------------------
def llm_chat(prompt, api_key, model="gpt-4o-mini", temperature=0.2, max_tokens=800, max_retries=3):
    client = OpenAI(api_key=api_key)
    
    for attempt in range(max_retries):
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "You are a helpful research assistant."},
                    {"role": "user", "content": prompt},
                ],
                temperature=temperature,
                max_tokens=max_tokens,
            )
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            error_str = str(e)
            
            # æª¢æŸ¥æ˜¯å¦æ˜¯é€Ÿç‡é™åˆ¶éŒ¯èª¤
            if "rate_limit_exceeded" in error_str or "Rate limit" in error_str:
                if attempt < max_retries - 1:
                    wait_time = 25 + (attempt * 10)  # éå¢ç­‰å¾…æ™‚é–“ï¼š25s, 35s, 45s
                    st.warning(f"API é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦... (å˜—è©¦ {attempt + 1}/{max_retries})")
                    sleep(wait_time)
                    continue
                else:
                    st.error("API é€Ÿç‡é™åˆ¶ï¼Œå·²é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸ã€‚è«‹ç¨å¾Œå†è©¦ã€‚")
                    return f"[LLM call failed] Rate limit exceeded after {max_retries} attempts"
            
            # å…¶ä»–éŒ¯èª¤
            elif "timeout" in error_str.lower():
                if attempt < max_retries - 1:
                    st.warning(f"API è«‹æ±‚è¶…æ™‚ï¼Œç­‰å¾… 10 ç§’å¾Œé‡è©¦... (å˜—è©¦ {attempt + 1}/{max_retries})")
                    sleep(10)
                    continue
            
            # å…¶ä»–éŒ¯èª¤ç›´æ¥è¿”å›
            return f"[LLM call failed] {e}"
    
    return f"[LLM call failed] Unknown error after {max_retries} attempts"

# ----------------------
# Data Collection Functions
# ----------------------
def search_arxiv(query, max_results=3, start_offset=0):
    """æœå°‹ arXiv æœ€æ–°è«–æ–‡"""
    
    # ç°¡åŒ–æœå°‹æŸ¥è©¢ï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦
    clean_query = query.replace('?', '').replace('!', '').replace('ã€‚', '')
    clean_query = clean_query.strip()
    
    try:
        # ä½¿ç”¨ HTTPS
        url = "https://export.arxiv.org/api/query"
        params = {
            'search_query': f'ti:{clean_query} OR abs:{clean_query}',
            'start': start_offset,
            'max_results': max_results,
            'sortBy': 'submittedDate',
            'sortOrder': 'descending'
        }
        
        # æ·»åŠ  headers
        headers = {
            'User-Agent': 'Mozilla/5.0 (compatible; Research Agent)',
            'Accept': 'application/atom+xml'
        }
        
        st.info(f"æ­£åœ¨æœå°‹ arXiv: {clean_query[:50]}...")
        response = requests.get(url, params=params, headers=headers, timeout=15)
        response.raise_for_status()
        
        feed = feedparser.parse(response.content)
        
        papers = []
        for entry in feed.entries:
            # æå–è«–æ–‡ ID
            paper_id = entry.id.split('/')[-1]
            
            # æ¸…ç†æ‘˜è¦
            summary = entry.summary.replace('\n', ' ').strip()
            if len(summary) > 500:
                summary = summary[:500] + "..."
            
            papers.append({
                'title': entry.title,
                'authors': [author.name for author in entry.authors],
                'summary': summary,
                'published': entry.published,
                'arxiv_id': paper_id,
                'url': entry.id,
                'categories': [tag.term for tag in entry.tags]
            })
        
        if papers:
            st.success(f"æˆåŠŸæ‰¾åˆ° {len(papers)} ç¯‡è«–æ–‡")
            return papers
        else:
            st.warning("æ²’æœ‰æ‰¾åˆ°ç›¸é—œè«–æ–‡")
            return []
            
    except Exception as e:
        st.warning(f"arXiv æœå°‹å¤±æ•—: {e}")
        return []


# ----------------------
# Agent functions
# ----------------------
def topic_refiner(topic, api_key):
    prompt = f"""ä½ æ˜¯ä¸€åè³‡æ·±ç ”ç©¶åŠ©ç†ã€‚é‡å°ä¸»é¡Œã€Œ{topic}ã€ï¼Œç”Ÿæˆ 3~5 å€‹å…·é«”ã€å¯ç ”ç©¶çš„å­å•é¡Œã€‚

            è«‹ç›´æ¥è¼¸å‡ºå•é¡Œæ¸…å–®ï¼Œæ¯è¡Œä¸€å€‹å•é¡Œï¼Œä¸è¦åŒ…å«ä»»ä½•æ ¼å¼ç¬¦è™Ÿæˆ– JSON çµæ§‹ã€‚

            ç¯„ä¾‹è¼¸å‡ºæ ¼å¼ï¼š
            æ©Ÿå™¨å­¸ç¿’ä¸­çš„æ³¨æ„åŠ›æ©Ÿåˆ¶å¦‚ä½•å·¥ä½œï¼Ÿ
            æ·±åº¦å­¸ç¿’åœ¨è‡ªç„¶èªè¨€è™•ç†ä¸­çš„æ‡‰ç”¨
            Transformer æ¶æ§‹çš„å„ªç¼ºé»åˆ†æ

            è«‹ç”Ÿæˆé—œæ–¼ã€Œ{topic}ã€çš„å…·é«”ç ”ç©¶å•é¡Œï¼š"""
    
    raw = llm_chat(prompt, api_key)
    
    # æ¸…ç†è¼¸å‡ºï¼Œç§»é™¤æ ¼å¼ç¬¦è™Ÿ
    lines = raw.split('\n')
    questions = []
    
    for i, line in enumerate(lines):
        line = line.strip()
        # ç§»é™¤å¸¸è¦‹çš„æ ¼å¼ç¬¦è™Ÿ
        line = line.replace('```json', '').replace('```', '').replace('{', '').replace('}', '').replace('[', '').replace(']', '')
        line = line.replace('"question":', '').replace('"', '').replace(',', '')
        
        # åªä¿ç•™çœ‹èµ·ä¾†åƒå•é¡Œçš„è¡Œ
        if line and len(line) > 10 and ('?' in line or 'å¦‚ä½•' in line or 'ä»€éº¼' in line or 'ç‚ºä»€éº¼' in line or 'å“ªäº›' in line or 'åˆ†æ' in line or 'ç ”ç©¶' in line):
            questions.append({
                "id": f"Q{i+1}",
                "question": line,
                "clarifier": f"é—œæ–¼ {topic} çš„ç ”ç©¶å•é¡Œ"
            })
    
    # å¦‚æœæ²’æœ‰æ‰¾åˆ°åˆé©çš„å•é¡Œï¼Œä½¿ç”¨å‚™ç”¨æ–¹æ¡ˆ
    if not questions:
        questions = [
            {"id": "Q1", "question": f"{topic}çš„åŸºæœ¬åŸç†æ˜¯ä»€éº¼ï¼Ÿ", "clarifier": f"é—œæ–¼ {topic} çš„åŸºæœ¬æ¦‚å¿µ"},
            {"id": "Q2", "question": f"{topic}çš„æœ€æ–°ç™¼å±•è¶¨å‹¢å¦‚ä½•ï¼Ÿ", "clarifier": f"é—œæ–¼ {topic} çš„ç™¼å±•ç¾æ³"},
            {"id": "Q3", "question": f"{topic}çš„æ‡‰ç”¨é ˜åŸŸæœ‰å“ªäº›ï¼Ÿ", "clarifier": f"é—œæ–¼ {topic} çš„å¯¦éš›æ‡‰ç”¨"}
        ]
    
    return questions[:5], raw


def researcher(questions, api_key):
    results = []
    for i, q in enumerate(questions):
        question = q['question']
        
        # æ¸…ç†å•é¡Œæ–‡æœ¬ï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦
        clean_question = question.replace('```', '').replace('json', '').replace('{', '').replace('}', '')
        clean_question = clean_question.replace('[', '').replace(']', '').replace('"', '').replace("'", '')
        clean_question = clean_question.replace('?', '').replace('!', '').replace('ã€‚', '').replace('ï¼Ÿ', '')
        clean_question = clean_question.strip()
        
        # ä½¿ç”¨å®Œæ•´çš„å•é¡Œä½œç‚ºæœå°‹æŸ¥è©¢ï¼Œä½†é™åˆ¶é•·åº¦
        search_query = clean_question
        if len(search_query) > 100:
            search_query = search_query[:100]
        
        if not clean_question or len(clean_question) < 5:
            st.warning(f"è·³éç„¡æ•ˆå•é¡Œ: {question}")
            continue
        
        # 1. æœå°‹ arXiv æœ€æ–°è«–æ–‡
        # ç‚ºæ¯å€‹å•é¡Œæ·»åŠ ä¸€äº›è®ŠåŒ–ï¼Œé¿å…é‡è¤‡çµæœ
        start_offset = random.randint(0, 5)  # éš¨æ©Ÿåç§»ï¼Œç²å–ä¸åŒçš„è«–æ–‡
        arxiv_papers = search_arxiv(search_query, max_results=3, start_offset=start_offset)
        if not arxiv_papers:
            st.info(f"arXiv ä¸­æ²’æœ‰æ‰¾åˆ°ã€Œ{clean_question}ã€çš„ç›¸é—œè«–æ–‡")
        
        # 2. æ•´ç†è³‡æ–™ç‚º JSON æ ¼å¼
        research_data = {
            'arxiv_papers': arxiv_papers,
            'search_timestamp': datetime.now().isoformat()
        }
        
        # 3. ä½¿ç”¨ LLM åˆ†æçœŸå¯¦è³‡æ–™
        # åœ¨æ¯å€‹å•é¡Œä¹‹é–“æ·»åŠ å»¶é²ï¼Œé¿å…é€Ÿç‡é™åˆ¶
        if i > 0:
            st.info("ç­‰å¾… 10 ç§’ä»¥é¿å… API é€Ÿç‡é™åˆ¶...")
            sleep(10)
        
        prompt = f"""ä½ æ˜¯ä¸€åç ”ç©¶å“¡ã€‚é‡å°å­å•é¡Œã€Œ{clean_question}ã€ï¼Œæˆ‘å·²ç¶“æ”¶é›†äº†çœŸå¯¦çš„ç ”ç©¶è³‡æ–™ï¼š

                arXiv è«–æ–‡è³‡æ–™ï¼š
                {json.dumps(arxiv_papers, ensure_ascii=False, indent=2)}

                è«‹æ ¹æ“šé€™äº›çœŸå¯¦çš„ arXiv è«–æ–‡è³‡æ–™é€²è¡Œåˆ†æï¼š
                1) æ•´ç† 2-3 æ¢å…·é«”çš„ç ”ç©¶ç™¼ç¾ï¼ˆåŸºæ–¼è«–æ–‡æ‘˜è¦å’Œæ¨™é¡Œï¼‰
                2) è©•ä¼°è³‡æ–™ä¾†æºçš„å¯ä¿¡åº¦ï¼ˆarXiv å­¸è¡“è«–æ–‡ï¼‰
                3) æä¾›ç°¡çŸ­çš„å¯ä¿¡åº¦è©•ä¼° (low/medium/high)
                4) å¦‚æœè³‡æ–™ä¸è¶³ï¼Œè«‹è¨»æ˜

                è¼¸å‡º JSON æ ¼å¼ï¼š
                {{\n                    "findings": ["åŸºæ–¼è«–æ–‡çš„ç™¼ç¾1", "åŸºæ–¼è«–æ–‡çš„ç™¼ç¾2"],\n                    "sources": ["arXivè«–æ–‡ä¾†æº"],\n                    "confidence": "medium",\n                    "data_quality": "good/limited",\n                    "notes": "é¡å¤–èªªæ˜æˆ–è³‡æ–™é™åˆ¶"\n                }}"""
        
        raw = llm_chat(prompt, api_key)
        try:
            parsed = json.loads(raw)
        except Exception:
            parsed = {
                "findings": [f"arXivè«–æ–‡: {paper['title'][:50]}..." for paper in arxiv_papers[:2]], 
                "sources": ["arXiv"],
                "confidence": "medium", 
                "data_quality": "good" if arxiv_papers else "limited",
                "notes": "JSON parsing failed, showing raw data",
                "__raw": raw
            }
        
        # æ·»åŠ åŸå§‹è³‡æ–™ä¾›åƒè€ƒ
        parsed['raw_research_data'] = research_data
        
        results.append({
            "id": q.get("id"), 
            "question": q.get("question"), 
            "analysis": parsed
        })
    
    return results


def summarizer(research_results, api_key, topic):
    prompt = f"ä½ æ˜¯ä¸€åç§‘å­¸ä½œå®¶ã€‚æ ¹æ“šä¸‹åˆ— Researcher è¼¸å‡ºçµæœç”Ÿæˆå®Œæ•´ç ”ç©¶å ±å‘Š:\nTopic: {topic}\nResults: {json.dumps(research_results, ensure_ascii=False, indent=2)}\n- Abstract 3è¡Œ\n- Introduction 1æ®µ\n- Key findings (æ•´åˆæ¯å€‹å­å•é¡Œ)\n- Limitations 1æ®µ\n- Recommended next steps 3 bullets\nå­—æ•¸ â‰¤600ã€‚"
    raw = llm_chat(prompt, api_key, temperature=0.15, max_tokens=1000)
    return raw

# ----------------------
# åˆå§‹åŒ– session state
# ----------------------
if 'research_completed' not in st.session_state:
    st.session_state.research_completed = False
if 'research_data' not in st.session_state:
    st.session_state.research_data = {}

# ----------------------
# Main UI
# ----------------------
if st.button("Start Research ğŸš€"):
    # æ¸…ç©ºä¹‹å‰çš„çµæœ
    st.session_state.research_completed = False
    st.session_state.research_data = {}
    
    st.subheader("ğŸ—ï¸ Topic Refiner")
    with st.spinner("Refining topic..."):
        questions, raw_build = topic_refiner(topic, api_key)
    st.write("**Refined questions:**")
    st.write(questions)
    st.write("**Raw output:**")
    st.code(raw_build[:3000])

    st.subheader("ğŸ“ Researcher")
    with st.spinner("Searching arXiv for latest papers..."):
        research_results = researcher(questions, api_key)
    
    # é¡¯ç¤ºè©³ç´°çš„ç ”ç©¶è³‡æ–™
    for i, result in enumerate(research_results):
        with st.expander(f"ç ”ç©¶å•é¡Œ {i+1}: {result['question']}"):
            analysis = result['analysis']
            
            st.write("**ç ”ç©¶ç™¼ç¾:**")
            for finding in analysis.get('findings', []):
                st.write(f"â€¢ {finding}")
            
            st.write("**è³‡æ–™ä¾†æº:**")
            for source in analysis.get('sources', []):
                st.write(f"â€¢ {source}")
            
            st.write(f"**å¯ä¿¡åº¦:** {analysis.get('confidence', 'unknown')}")
            st.write(f"**è³‡æ–™å“è³ª:** {analysis.get('data_quality', 'unknown')}")
            
            # é¡¯ç¤ºåŸå§‹è³‡æ–™
            raw_data = analysis.get('raw_research_data', {})
            if raw_data.get('arxiv_papers'):
                st.write("**arXiv è«–æ–‡:**")
                for paper in raw_data['arxiv_papers']:
                    st.write(f"ğŸ“„ [{paper['title']}]({paper['url']})")
                    st.write(f"   {paper['summary'][:200]}...")
                    st.write(f"   ğŸ‘¥ ä½œè€…: {', '.join(paper['authors'][:3])}{'...' if len(paper['authors']) > 3 else ''}")
                    st.write(f"   ğŸ“… ç™¼å¸ƒ: {paper['published'][:10]}")
                    st.write("---")
    
    st.write("**å®Œæ•´ç ”ç©¶çµæœ:**")
    st.write(research_results)

    st.subheader("âœï¸ Summarizer")
    with st.spinner("Generating final report..."):
        # åœ¨ç”Ÿæˆå ±å‘Šå‰æ·»åŠ å»¶é²
        sleep(10)
        final_report = summarizer(research_results, api_key, topic)
    st.markdown(final_report)

    # ä¸‹è¼‰æŒ‰éˆ•
    st.download_button("Download report (.txt)", data=final_report, file_name="research_report.txt")
    st.download_button("Download JSON export", data=json.dumps({"topic": topic, "questions": questions, "research": research_results, "report": final_report}, ensure_ascii=False, indent=2), file_name="research_export.json")
    
    # å„²å­˜çµæœåˆ° session state
    st.session_state.research_data = {
        'questions': questions,
        'raw_build': raw_build,
        'research_results': research_results,
        'final_report': final_report,
        'topic': topic
    }
    st.session_state.research_completed = True

# å¦‚æœæœ‰ç ”ç©¶çµæœä¸”ä¸æ˜¯å‰›å®Œæˆçš„ï¼Œé¡¯ç¤ºçµæœ
elif st.session_state.research_completed and st.session_state.research_data:
    data = st.session_state.research_data
    
    st.subheader("ğŸ—ï¸ Topic Refiner")
    st.write("**Refined questions:**")
    st.write(data['questions'])
    st.write("**Raw output:**")
    st.code(data['raw_build'][:3000])

    st.subheader("ğŸ“ Researcher")
    
    # é¡¯ç¤ºè©³ç´°çš„ç ”ç©¶è³‡æ–™
    for i, result in enumerate(data['research_results']):
        with st.expander(f"ç ”ç©¶å•é¡Œ {i+1}: {result['question']}"):
            analysis = result['analysis']
            
            st.write("**ç ”ç©¶ç™¼ç¾:**")
            for finding in analysis.get('findings', []):
                st.write(f"â€¢ {finding}")
            
            st.write("**è³‡æ–™ä¾†æº:**")
            for source in analysis.get('sources', []):
                st.write(f"â€¢ {source}")
            
            st.write(f"**å¯ä¿¡åº¦:** {analysis.get('confidence', 'unknown')}")
            st.write(f"**è³‡æ–™å“è³ª:** {analysis.get('data_quality', 'unknown')}")
            
            # é¡¯ç¤ºåŸå§‹è³‡æ–™
            raw_data = analysis.get('raw_research_data', {})
            if raw_data.get('arxiv_papers'):
                st.write("**arXiv è«–æ–‡:**")
                for paper in raw_data['arxiv_papers']:
                    st.write(f"ğŸ“„ [{paper['title']}]({paper['url']})")
                    st.write(f"   {paper['summary'][:200]}...")
                    st.write(f"   ğŸ‘¥ ä½œè€…: {', '.join(paper['authors'][:3])}{'...' if len(paper['authors']) > 3 else ''}")
                    st.write(f"   ğŸ“… ç™¼å¸ƒ: {paper['published'][:10]}")
                    st.write("---")
    
    st.write("**å®Œæ•´ç ”ç©¶çµæœ:**")
    st.write(data['research_results'])

    st.subheader("âœï¸ Summarizer")
    st.markdown(data['final_report'])

    # ä¸‹è¼‰æŒ‰éˆ•
    st.download_button("Download report (.txt)", data=data['final_report'], file_name="research_report.txt")
    st.download_button("Download JSON export", data=json.dumps({"topic": data['topic'], "questions": data['questions'], "research": data['research_results'], "report": data['final_report']}, ensure_ascii=False, indent=2), file_name="research_export.json")
