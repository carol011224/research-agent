import streamlit as st
from openai import OpenAI
import json
import os
from dotenv import load_dotenv

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

st.set_page_config(page_title="A Research Agent Demo", layout="wide")
st.title("A Research Agent Demo")
st.write("Three sub-agents: Topic Refiner â†’ Researcher â†’ Summarizer")

# ----------------------
# æª¢æŸ¥ API Key
# ----------------------
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    st.error("âŒ ç„¡æ³•è®€å– .env æª”æ¡ˆæˆ–æ‰¾ä¸åˆ° OPENAI_API_KEY")
    st.stop()

# ----------------------
# Sidebar: ä¸»é¡Œè¨­å®š
# ----------------------
with st.sidebar:
    topic = st.text_input("Research topic", value="æœ€æ–°å¤šèªè¨€ LLM fine-tuning æ–¹æ³•")
    temp = st.slider("Temperature", 0.0, 1.0, 0.2)
    st.markdown("---")

# ----------------------
# LLM wrapper
# ----------------------
def llm_chat(prompt, api_key, model="gpt-4o-mini", temperature=0.2, max_tokens=800):
    client = OpenAI(api_key=api_key)
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "You are a helpful research assistant."},
                {"role": "user", "content": prompt},
            ],
            temperature=temperature,
            max_tokens=max_tokens,
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"[LLM call failed] {e}"

# ----------------------
# Agent functions
# ----------------------
def topic_refiner(topic, api_key):
    prompt = f"ä½ æ˜¯ä¸€åè³‡æ·±ç ”ç©¶åŠ©ç†ã€‚é‡å°ä¸»é¡Œã€Œ{topic}ã€ï¼Œç”Ÿæˆ 3~5 å€‹å…·é«”ã€å¯ç ”ç©¶çš„å­å•é¡Œï¼Œä¸¦ç‚ºæ¯å€‹å­å•é¡Œé™„ä¸Šç°¡çŸ­ clarifierã€‚è¼¸å‡º JSON æ ¼å¼ã€‚"
    raw = llm_chat(prompt, api_key)
    try:
        parsed = json.loads(raw)
    except Exception:
        parsed = [{"id": f"Q{i+1}", "question": line[:200], "clarifier": ""} for i, line in enumerate(raw.splitlines()[:5])]
    return parsed, raw


def researcher(questions, api_key):
    results = []
    for q in questions:
        prompt = f"ä½ æ˜¯ä¸€åç ”ç©¶å“¡ã€‚é‡å°å­å•é¡Œ {q['question']}ï¼š\n1) æ¨¡æ“¬ 2~3 æ¢æœ€æ–°ç ”ç©¶ç™¼ç¾ï¼Œæ¯æ¢ 1 å¥ã€‚\n2) æ¨¡æ“¬å¯èƒ½è³‡æ–™ä¾†æºï¼ˆå¦‚ arXiv, gov reportï¼‰\n3) ç°¡çŸ­å¯ä¿¡åº¦è©•ä¼° (low/medium/high)ã€‚è¼¸å‡º JSONã€‚"
        raw = llm_chat(prompt, api_key)
        try:
            parsed = json.loads(raw)
        except Exception:
            parsed = {"findings": [s.strip() for s in raw.splitlines()[:3]], "sources_hint":"unspecified", "confidence":"unknown", "__raw": raw}
        results.append({"id": q.get("id"), "question": q.get("question"), "analysis": parsed})
    return results


def summarizer(research_results, api_key, topic):
    prompt = f"ä½ æ˜¯ä¸€åç§‘å­¸ä½œå®¶ã€‚æ ¹æ“šä¸‹åˆ— Researcher è¼¸å‡ºçµæœç”Ÿæˆå®Œæ•´ç ”ç©¶å ±å‘Š:\nTopic: {topic}\nResults: {json.dumps(research_results, ensure_ascii=False, indent=2)}\n- Abstract 3è¡Œ\n- Introduction 1æ®µ\n- Key findings (æ•´åˆæ¯å€‹å­å•é¡Œ)\n- Limitations 1æ®µ\n- Recommended next steps 3 bullets\nå­—æ•¸ â‰¤600ã€‚"
    raw = llm_chat(prompt, api_key, temperature=0.15, max_tokens=1000)
    return raw

# ----------------------
# åˆå§‹åŒ– session state
# ----------------------
if 'research_completed' not in st.session_state:
    st.session_state.research_completed = False
if 'research_data' not in st.session_state:
    st.session_state.research_data = {}

# ----------------------
# Main UI
# ----------------------
if st.button("Start Research ğŸš€"):
    # æ¸…ç©ºä¹‹å‰çš„çµæœ
    st.session_state.research_completed = False
    st.session_state.research_data = {}
    
    st.subheader("ğŸ—ï¸ Topic Refiner")
    with st.spinner("Refining topic..."):
        questions, raw_build = topic_refiner(topic, api_key)
    st.write("**Refined questions:**")
    st.write(questions)
    st.write("**Raw output:**")
    st.code(raw_build[:3000])

    st.subheader("ğŸ“ Researcher")
    with st.spinner("Simulating research findings..."):
        research_results = researcher(questions, api_key)
    st.write("**Research results:**")
    st.write(research_results)

    st.subheader("âœï¸ Summarizer")
    with st.spinner("Generating final report..."):
        final_report = summarizer(research_results, api_key, topic)
    st.markdown(final_report)

    # ä¸‹è¼‰æŒ‰éˆ•
    st.download_button("Download report (.txt)", data=final_report, file_name="research_report.txt")
    st.download_button("Download JSON export", data=json.dumps({"topic": topic, "questions": questions, "research": research_results, "report": final_report}, ensure_ascii=False, indent=2), file_name="research_export.json")
    
    # å„²å­˜çµæœåˆ° session state
    st.session_state.research_data = {
        'questions': questions,
        'raw_build': raw_build,
        'research_results': research_results,
        'final_report': final_report,
        'topic': topic
    }
    st.session_state.research_completed = True

# å¦‚æœæœ‰ç ”ç©¶çµæœä¸”ä¸æ˜¯å‰›å®Œæˆçš„ï¼Œé¡¯ç¤ºçµæœ
elif st.session_state.research_completed and st.session_state.research_data:
    data = st.session_state.research_data
    
    st.subheader("ğŸ—ï¸ Topic Refiner")
    st.write("**Refined questions:**")
    st.write(data['questions'])
    st.write("**Raw output:**")
    st.code(data['raw_build'][:3000])

    st.subheader("ğŸ“ Researcher")
    st.write("**Research results:**")
    st.write(data['research_results'])

    st.subheader("âœï¸ Summarizer")
    st.markdown(data['final_report'])

    # ä¸‹è¼‰æŒ‰éˆ•
    st.download_button("Download report (.txt)", data=data['final_report'], file_name="research_report.txt")
    st.download_button("Download JSON export", data=json.dumps({"topic": data['topic'], "questions": data['questions'], "research": data['research_results'], "report": data['final_report']}, ensure_ascii=False, indent=2), file_name="research_export.json")
