import streamlit as st
from openai import OpenAI
import json
import os
from dotenv import load_dotenv

# LangChain å°å…¥
from langchain_openai import ChatOpenAI
from typing import Dict, Any, List

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

st.set_page_config(page_title="LangChain Research Agent", layout="wide")
st.title("ğŸ¤– LangChain Research Agent")
st.write("AI Agent ç ”ç©¶ç³»çµ±ï¼šTopic Refiner â†’ Researcher â†’ Summarizer")

# ----------------------
# æª¢æŸ¥ API Key
# ----------------------
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    st.error("âŒ ç„¡æ³•è®€å– .env æª”æ¡ˆæˆ–æ‰¾ä¸åˆ° OPENAI_API_KEY")
    st.stop()

# ----------------------
# Sidebar: ä¸»é¡Œè¨­å®š
# ----------------------
with st.sidebar:
    topic = st.text_input("Research topic", value="æœ€æ–° RAG çš„æ–¹æ³•")
    st.markdown("---")

# ----------------------
# LangChain è¨­å®š
# ----------------------
llm = ChatOpenAI(
    api_key=api_key,
    model="gpt-4o-mini",
    temperature=0.1,
    streaming=False
)

# ----------------------
# LLM wrapper and tools
# ----------------------
def llm_chat(prompt, api_key, model="gpt-4o-mini", temperature=0.1, max_tokens=800):
    client = OpenAI(api_key=api_key)
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "You are a helpful research assistant."},
                {"role": "user", "content": prompt},
            ],
            temperature=temperature,
            max_tokens=max_tokens,
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"[LLM call failed] {e}"

def topic_refiner(topic, api_key):
    prompt = f"ä½ æ˜¯ä¸€åè³‡æ·±ç ”ç©¶åŠ©ç†ã€‚é‡å°ä¸»é¡Œã€Œ{topic}ã€ï¼Œç”Ÿæˆ 3~5 å€‹å…·é«”ã€å¯ç ”ç©¶çš„å­å•é¡Œï¼Œä¸¦ç‚ºæ¯å€‹å­å•é¡Œé™„ä¸Šç°¡çŸ­ clarifierã€‚è¼¸å‡º JSON æ ¼å¼ã€‚"
    raw = llm_chat(prompt, api_key)
    
    # æ¸…ç† raw è¼¸å‡ºï¼Œç§»é™¤å¯èƒ½çš„ markdown æ¨™è¨˜
    clean_raw = raw.strip()
    if clean_raw.startswith("```json"):
        clean_raw = clean_raw[7:]  # ç§»é™¤ ```json
    if clean_raw.startswith("```"):
        clean_raw = clean_raw[3:]   # ç§»é™¤ ```
    if clean_raw.endswith("```"):
        clean_raw = clean_raw[:-3]  # ç§»é™¤çµå°¾çš„ ```
    clean_raw = clean_raw.strip()
    
    try:
        parsed = json.loads(clean_raw)
        
        # è™•ç†å¯èƒ½çš„ sub_questions çµæ§‹
        if isinstance(parsed, dict) and 'sub_questions' in parsed:
            questions_list = parsed['sub_questions']
        elif isinstance(parsed, list):
            questions_list = parsed
        else:
            questions_list = []
        
        # è½‰æ›ç‚ºæ¨™æº–æ ¼å¼
        formatted_questions = []
        for i, q in enumerate(questions_list, 1):
            if isinstance(q, dict):
                formatted_questions.append({
                    "id": f"Q{i}",
                    "question": q.get("question", ""),
                    "clarifier": q.get("clarifier", "")
                })
        
        if not formatted_questions:
            raise ValueError("No valid questions found")
            
        return formatted_questions, raw
        
    except Exception as e:
        # å¦‚æœ JSON è§£æå¤±æ•—ï¼Œå˜—è©¦æå–å•é¡Œ
        questions = []
        for line in raw.splitlines():
            line = line.strip()
            # å°‹æ‰¾åŒ…å« "question" çš„è¡Œ
            if '"question"' in line:
                try:
                    # æå–å•é¡Œæ–‡å­—
                    question_match = line.split('"question"')[1].split('"')[1]
                    if question_match and len(question_match) > 10:
                        questions.append({
                            "id": f"Q{len(questions)+1}",
                            "question": question_match,
                            "clarifier": ""
                        })
                except:
                    pass
        
        if questions:
            return questions, raw
        else:
            # æœ€å¾Œçš„å‚™ç”¨æ–¹æ¡ˆ
            fallback_questions = [
                {"id": "Q1", "question": f"{topic}çš„åŸºæœ¬åŸç†æ˜¯ä»€éº¼ï¼Ÿ", "clarifier": f"é—œæ–¼ {topic} çš„åŸºæœ¬æ¦‚å¿µ"},
                {"id": "Q2", "question": f"{topic}çš„æœ€æ–°ç™¼å±•è¶¨å‹¢å¦‚ä½•ï¼Ÿ", "clarifier": f"é—œæ–¼ {topic} çš„ç™¼å±•ç¾æ³"},
                {"id": "Q3", "question": f"{topic}çš„æ‡‰ç”¨é ˜åŸŸæœ‰å“ªäº›ï¼Ÿ", "clarifier": f"é—œæ–¼ {topic} çš„å¯¦éš›æ‡‰ç”¨"}
            ]
            return fallback_questions, raw

def search_arxiv(query, max_results=3, start_offset=0):
    """æœå°‹ arXiv æœ€æ–°è«–æ–‡"""
    try:
        import requests
        import feedparser
        
        # ç°¡åŒ–æœå°‹æŸ¥è©¢ï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦
        clean_query = query.replace('?', '').replace('!', '').replace('ã€‚', '').replace('ï¼Ÿ', '')
        clean_query = clean_query.strip()
        
        # æå–é—œéµå­—ï¼ˆå¦‚æœæŸ¥è©¢å¤ªé•·ï¼Œåªå–å‰å¹¾å€‹é—œéµå­—ï¼‰
        query_words = clean_query.split()
        if len(query_words) > 5:
            # å¦‚æœæŸ¥è©¢å¤ªé•·ï¼Œåªä½¿ç”¨å‰ 5 å€‹å­—ä½œç‚ºé—œéµå­—
            clean_query = ' '.join(query_words[:5])
        
        # ä½¿ç”¨ HTTPS
        url = "https://export.arxiv.org/api/query"
        # æ”¹ç”¨æ›´ç²¾ç¢ºçš„æœå°‹ï¼šå„ªå…ˆæ¨™é¡Œï¼Œç„¶å¾Œæ‘˜è¦
        params = {
            'search_query': f'ti:{clean_query} OR abs:{clean_query}',
            'start': start_offset,
            'max_results': max_results,
            'sortBy': 'submittedDate',
            'sortOrder': 'descending'
        }
        
        # æ·»åŠ  headers
        headers = {
            'User-Agent': 'Mozilla/5.0 (compatible; Research Agent)',
            'Accept': 'application/atom+xml'
        }
        
        response = requests.get(url, params=params, headers=headers, timeout=15)
        response.raise_for_status()
        
        feed = feedparser.parse(response.content)
        
        papers = []
        for entry in feed.entries:
            # æå–è«–æ–‡ ID
            paper_id = entry.id.split('/')[-1]
            
            # æ¸…ç†æ‘˜è¦
            summary = entry.summary.replace('\n', ' ').strip()
            if len(summary) > 500:
                summary = summary[:500] + "..."
            
            papers.append({
                'title': entry.title,
                'authors': [author.name for author in entry.authors],
                'summary': summary,
                'published': entry.published,
                'arxiv_id': paper_id,
                'url': entry.id,
                'categories': [tag.term for tag in entry.tags]
            })
        
        return papers
        
    except Exception as e:
        print(f"arXiv æœå°‹å¤±æ•—: {e}")
        return []

def researcher(questions, api_key):
    """çœŸæ­£çš„ç ”ç©¶å“¡ - æœå°‹çœŸå¯¦çš„ arXiv è«–æ–‡"""
    results = []
    seen_paper_ids = set()  # è¨˜éŒ„å·²çœ‹éçš„è«–æ–‡ IDï¼Œé¿å…é‡è¤‡
    
    for i, q in enumerate(questions):
        question = q['question']
        
        # 1. æœå°‹ arXiv çœŸå¯¦è«–æ–‡
        # ç‚ºæ¯å€‹å•é¡Œä½¿ç”¨ä¸åŒçš„ start_offsetï¼Œé¿å…è¿”å›ç›¸åŒè«–æ–‡
        start_offset = i * 5  # å¢åŠ é–“è·ï¼Œç¢ºä¿æ¯å€‹å•é¡Œç²å–ä¸åŒçš„è«–æ–‡
        arxiv_papers = search_arxiv(question, max_results=5, start_offset=start_offset)
        
        # éæ¿¾æ‰å·²ç¶“çœ‹éçš„è«–æ–‡
        unique_papers = []
        for paper in arxiv_papers:
            paper_id = paper.get('arxiv_id', '')
            if paper_id and paper_id not in seen_paper_ids:
                unique_papers.append(paper)
                seen_paper_ids.add(paper_id)
        
        # å¦‚æœéæ¿¾å¾Œæ²’æœ‰è«–æ–‡ï¼Œå†æœå°‹ä¸€æ¬¡ï¼ˆä¸ä½¿ç”¨ start_offsetï¼‰
        if not unique_papers:
            fallback_papers = search_arxiv(question, max_results=5, start_offset=0)
            for paper in fallback_papers:
                paper_id = paper.get('arxiv_id', '')
                if paper_id and paper_id not in seen_paper_ids:
                    unique_papers.append(paper)
                    seen_paper_ids.add(paper_id)
        
        # åªå–å‰ 3 ç¯‡è«–æ–‡
        arxiv_papers = unique_papers[:3]
        
        # 2. ä½¿ç”¨ LLM åˆ†æçœŸå¯¦è«–æ–‡è³‡æ–™
        if arxiv_papers:
            prompt = f"""ä½ æ˜¯ä¸€åç ”ç©¶å“¡ã€‚é‡å°å­å•é¡Œã€Œ{question}ã€ï¼Œæˆ‘å·²ç¶“æœå°‹åˆ°ä»¥ä¸‹çœŸå¯¦çš„ arXiv è«–æ–‡ï¼š

{json.dumps(arxiv_papers, ensure_ascii=False, indent=2)}

è«‹æ ¹æ“šé€™äº›çœŸå¯¦è«–æ–‡é€²è¡Œåˆ†æï¼š
1) æå– 2-3 æ¢å…·é«”çš„ç ”ç©¶ç™¼ç¾ï¼ˆåŸºæ–¼è«–æ–‡æ‘˜è¦å’Œæ¨™é¡Œï¼‰
2) è©•ä¼°è³‡æ–™ä¾†æºçš„å¯ä¿¡åº¦ï¼ˆarXiv å­¸è¡“è«–æ–‡ï¼‰
3) æä¾›å¯ä¿¡åº¦è©•ä¼° (low/medium/high)
4) å¦‚æœè³‡æ–™ä¸è¶³ï¼Œè«‹è¨»æ˜

è¼¸å‡º JSON æ ¼å¼ï¼š
{{
    "findings": ["åŸºæ–¼è«–æ–‡çš„ç™¼ç¾1", "åŸºæ–¼è«–æ–‡çš„ç™¼ç¾2"],
    "sources": ["arXivè«–æ–‡ä¾†æº"],
    "confidence": "medium",
    "data_quality": "good/limited",
    "notes": "é¡å¤–èªªæ˜æˆ–è³‡æ–™é™åˆ¶"
}}"""
        else:
            prompt = f"""ä½ æ˜¯ä¸€åç ”ç©¶å“¡ã€‚é‡å°å­å•é¡Œã€Œ{question}ã€ï¼Œæˆ‘æ²’æœ‰æ‰¾åˆ°ç›¸é—œçš„ arXiv è«–æ–‡ã€‚

è«‹æä¾›ï¼š
1) åŸºæ–¼ä¸€èˆ¬çŸ¥è­˜çš„ 2-3 æ¢ç›¸é—œç™¼ç¾
2) è¨»æ˜è³‡æ–™ä¾†æºé™åˆ¶
3) æä¾›å¯ä¿¡åº¦è©•ä¼° (low/medium/high)

è¼¸å‡º JSON æ ¼å¼ï¼š
{{
    "findings": ["ä¸€èˆ¬çŸ¥è­˜ç™¼ç¾1", "ä¸€èˆ¬çŸ¥è­˜ç™¼ç¾2"],
    "sources": ["ä¸€èˆ¬çŸ¥è­˜"],
    "confidence": "low",
    "data_quality": "limited",
    "notes": "æœªæ‰¾åˆ°ç›¸é—œå­¸è¡“è«–æ–‡ï¼ŒåŸºæ–¼ä¸€èˆ¬çŸ¥è­˜"
}}"""
        
        raw = llm_chat(prompt, api_key)
        try:
            parsed = json.loads(raw)
        except Exception:
            parsed = {
                "findings": [f"arXivè«–æ–‡: {paper['title'][:50]}..." for paper in arxiv_papers[:2]] if arxiv_papers else ["æœªæ‰¾åˆ°ç›¸é—œè«–æ–‡"],
                "sources": ["arXiv"] if arxiv_papers else ["ä¸€èˆ¬çŸ¥è­˜"],
                "confidence": "medium" if arxiv_papers else "low",
                "data_quality": "good" if arxiv_papers else "limited",
                "notes": "JSON parsing failed, showing raw data",
                "__raw": raw
            }
        
        # æ·»åŠ åŸå§‹è«–æ–‡è³‡æ–™
        parsed['arxiv_papers'] = arxiv_papers
        
        results.append({
            "id": q.get("id"), 
            "question": q.get("question"), 
            "analysis": parsed
        })
    
    return results

def summarizer(research_results, api_key, topic):
    prompt = f"ä½ æ˜¯ä¸€åç§‘å­¸ä½œå®¶ã€‚æ ¹æ“šä¸‹åˆ— Researcher è¼¸å‡ºçµæœç”Ÿæˆå®Œæ•´ç ”ç©¶å ±å‘Š:\nTopic: {topic}\nResults: {json.dumps(research_results, ensure_ascii=False, indent=2)}\n- Abstract 3è¡Œ\n- Introduction 1æ®µ\n- Key findings (æ•´åˆæ¯å€‹å­å•é¡Œ)\n- Limitations 1æ®µ\n- Recommended next steps 3 bullets\nå­—æ•¸ â‰¤600ã€‚"
    raw = llm_chat(prompt, api_key, temperature=0.15, max_tokens=1000)
    return raw


# ----------------------
# LangChain Agent å®šç¾©ï¼ˆç°¡åŒ–ç‰ˆï¼‰
# ----------------------
def create_research_agent():
    """å»ºç«‹ç ”ç©¶ä»£ç†ï¼ˆä¿®å¾©ç‰ˆï¼‰"""
    def agent_run(topic: str) -> dict:
        """åŸ·è¡Œç ”ç©¶æµç¨‹ï¼Œè¿”å›åŸå§‹è³‡æ–™çµæ§‹"""
        # æ­¥é©Ÿ 1: ä¸»é¡Œç´°åŒ–
        questions, raw_build = topic_refiner(topic, api_key)
        
        # æ­¥é©Ÿ 2: ç ”ç©¶å•é¡Œ
        research_results = researcher(questions, api_key)
        
        # æ­¥é©Ÿ 3: ç”Ÿæˆå ±å‘Š
        final_report = summarizer(research_results, api_key, topic)
        
        # è¿”å›åŸå§‹è³‡æ–™çµæ§‹ï¼Œä¸é€²è¡Œ JSON åºåˆ—åŒ–
        return {
            "questions": questions,
            "research_results": research_results,
            "final_report": final_report,
            "raw_build": raw_build,
            "topic": topic
        }
    
    return type('SimpleAgent', (), {'invoke': lambda self, inputs: agent_run(inputs['topic'])})()


# ----------------------
# åˆå§‹åŒ– session state
# ----------------------
if 'research_completed' not in st.session_state:
    st.session_state.research_completed = False
if 'research_data' not in st.session_state:
    st.session_state.research_data = {}
if 'current_topic' not in st.session_state:
    st.session_state.current_topic = ""

# ----------------------
# Main UI
# ----------------------
if st.button("Start Research ğŸš€"):
    # æ¸…ç©ºä¹‹å‰çš„çµæœ
    st.session_state.research_completed = False
    st.session_state.research_data = {}
    st.session_state.current_topic = topic
    
    # ä½¿ç”¨ LangChain Agent
    st.subheader("ğŸ¤– LangChain Agent åŸ·è¡Œ")
    
    # å‰µå»ºé€²åº¦æ¢
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    try:
        status_text.text("ğŸ”„ æ­£åœ¨åˆå§‹åŒ– Agent...")
        progress_bar.progress(10)
        
        agent = create_research_agent()
        
        status_text.text("ğŸ” æ­£åœ¨åŸ·è¡Œç ”ç©¶æµç¨‹...")
        progress_bar.progress(30)
        
        result = agent.invoke({"topic": topic})
        
        status_text.text("âœ… Agent åŸ·è¡Œå®Œæˆï¼")
        progress_bar.progress(100)
        
    except Exception as e:
        status_text.text(f"âŒ Agent åŸ·è¡Œå¤±æ•—: {str(e)}")
        progress_bar.progress(0)
        st.error(f"Agent åŸ·è¡ŒéŒ¯èª¤: {str(e)}")
        st.stop()
    
    # è§£æçµæœï¼ˆä¿®å¾©ç‰ˆï¼‰
    result_data = result  # ç›´æ¥ä½¿ç”¨çµæœï¼Œä¸éœ€è¦ JSON è§£æ
    
    questions = result_data.get("questions", [])
    raw_build = result_data.get("raw_build", "")
    research_results = result_data.get("research_results", [])
    final_report = result_data.get("final_report", "")
    
    st.subheader("ğŸ—ï¸ Topic Refiner")
    st.markdown("### ğŸ“‹ ç´°åŒ–çš„ç ”ç©¶å•é¡Œ")
    
    # ç°¡å–®æ¸…æ¥šçš„æ ¼å¼
    if questions:
        for i, q in enumerate(questions, 1):
            question_id = q.get('id', f'Q{i}')
            question_text = q.get('question', '')
            clarifier_text = q.get('clarifier', '')
            
            st.write(f"**ğŸ“Œ {question_id}:** {question_text}")
            if clarifier_text:
                st.write(f"ğŸ’¡ {clarifier_text}")
            st.write("")  # ç©ºè¡Œåˆ†éš”
    
    # å¯å±•é–‹çš„åŸå§‹è¼¸å‡º
    with st.expander("ğŸ” æŸ¥çœ‹åŸå§‹è¼¸å‡º (Raw Output)", expanded=False):
        st.code(raw_build[:3000], language="json")

    st.subheader("ğŸ“ Researcher")
    st.write("**Research results:**")
    
    # é¡¯ç¤ºè©³ç´°çš„ç ”ç©¶è³‡æ–™
    for i, result in enumerate(research_results):
        with st.expander(f"ç ”ç©¶å•é¡Œ {i+1}: {result['question']}"):
            analysis = result['analysis']
            
            st.write("**ç ”ç©¶ç™¼ç¾:**")
            for finding in analysis.get('findings', []):
                st.write(f"â€¢ {finding}")
            
            st.write("**è³‡æ–™ä¾†æº:**")
            for source in analysis.get('sources', []):
                st.write(f"â€¢ {source}")
            
            st.write(f"**å¯ä¿¡åº¦:** {analysis.get('confidence', 'unknown')}")
            st.write(f"**è³‡æ–™å“è³ª:** {analysis.get('data_quality', 'unknown')}")
            
            # é¡¯ç¤ºçœŸå¯¦çš„ arXiv è«–æ–‡
            arxiv_papers = analysis.get('arxiv_papers', [])
            if arxiv_papers:
                st.write("**ğŸ“š arXiv è«–æ–‡:**")
                for paper in arxiv_papers:
                    st.write(f"ğŸ“„ **[{paper['title']}]({paper['url']})**")
                    st.write(f"   ğŸ‘¥ ä½œè€…: {', '.join(paper['authors'][:3])}{'...' if len(paper['authors']) > 3 else ''}")
                    st.write(f"   ğŸ“… ç™¼å¸ƒ: {paper['published'][:10]}")
                    st.write(f"   ğŸ“ æ‘˜è¦: {paper['summary'][:500]}...")
                    st.write(f"   ğŸ·ï¸ åˆ†é¡: {', '.join(paper['categories'][:3])}")
                    st.write("---")
            else:
                st.write("**âš ï¸ æœªæ‰¾åˆ°ç›¸é—œ arXiv è«–æ–‡**")
    
    # å®Œæ•´ç ”ç©¶çµæœï¼ˆé è¨­æ”¶èµ·ä¾†ï¼‰
    with st.expander("ğŸ“Š å®Œæ•´ç ”ç©¶çµæœ (JSON)", expanded=False):
        st.write(research_results)

    st.subheader("âœï¸ Summarizer")
    st.markdown(final_report)
    
    # å„²å­˜çµæœ
    st.session_state.research_data = {
        'questions': questions,
        'raw_build': raw_build,
        'research_results': research_results,
        'final_report': final_report,
        'topic': topic,
        'agent_output': result
    }
    
    # ä¸‹è¼‰æŒ‰éˆ•
    st.download_button("Download report (.txt)", data=st.session_state.research_data['final_report'], file_name="research_report.txt")
    st.download_button("Download JSON export", data=json.dumps(st.session_state.research_data, ensure_ascii=False, indent=2), file_name="research_export.json")
    
    st.session_state.research_completed = True

# å¦‚æœæœ‰ç ”ç©¶çµæœä¸”ä¸æ˜¯å‰›å®Œæˆçš„ï¼Œé¡¯ç¤ºçµæœ
elif st.session_state.research_completed and st.session_state.research_data:
    data = st.session_state.research_data
    
    st.subheader("ğŸ—ï¸ Topic Refiner")
    
    # ç°¡å–®æ¸…æ¥šçš„æ ¼å¼
    if data['questions']:
        for i, q in enumerate(data['questions'], 1):
            question_id = q.get('id', f'Q{i}')
            question_text = q.get('question', '')
            clarifier_text = q.get('clarifier', '')
            
            st.write(f"**ğŸ“Œ {question_id}:** {question_text}")
            if clarifier_text:
                st.write(f"ğŸ’¡ {clarifier_text}")
            st.write("")  # ç©ºè¡Œåˆ†éš”
    
    # å¯å±•é–‹çš„åŸå§‹è¼¸å‡º
    with st.expander("ğŸ” æŸ¥çœ‹åŸå§‹è¼¸å‡º (Raw Output)", expanded=False):
        st.code(data['raw_build'][:3000], language="json")

    st.subheader("ğŸ“ Researcher")
    with st.expander("ğŸ“Š å®Œæ•´ç ”ç©¶çµæœ (JSON)", expanded=False):
        st.write(data['research_results'])

    st.subheader("âœï¸ Summarizer")
    st.markdown(data['final_report'])

    # ä¸‹è¼‰æŒ‰éˆ•
    st.download_button("Download report (.txt)", data=data['final_report'], file_name="research_report.txt")
    st.download_button("Download JSON export", data=json.dumps(data, ensure_ascii=False, indent=2), file_name="research_export.json")
